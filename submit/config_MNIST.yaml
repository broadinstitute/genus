# neptune project for logging the simulation results
neptune_project: "cellarium/genus"

# simulation-level parameters
simulation:
  # can be scratch, resume, pretrained
  type: "scratch"
  BATCH_SIZE: 400
  MAX_EPOCHS: 1601
  TEST_FREQUENCY: 10
  CHECKPOINT_FREQUENCY: 50

# architecture parameters
architecture:

  # Ingeters specifying the unet architecture
  # Scale factor need to be power of 2 (i.e. 1,2,4,8,16,...) and describe the downsampling in spatial resolution
  # If the input image is 256x256 and unet_scale_factor_boundigboxes=32
  # then the bounding_boxes are defined on a 8x8 grid
  space_inspired: True
  unet_ch_feature_map: 1
  pretrained_unet_from_mateuszbuda: True
  partially_frozen_unet: False
  unet_scale_factor_boundingboxes: 8

  # The following values are used only if pretrained_unet_from_mateuszbuda == False
  unet_scale_factor_initial_layer: 1  # at the moment only 1 will work
  unet_scale_factor_background: 16
  unet_ch_before_first_maxpool: 8

  # Integers. Latent dimension for z_background and z_where
  # Note that the background latent dimension is defined on a 2D grid. It therefore has implicitely positional information
  # If zbg_dim is too high the object can be reconstructed using the background.
  # Possible solutions:
  # 1. Use low zbg_dim
  # 2. Use a fc encoder-decoder for the background
  zbg_dim: 10  # small value otherwise the background can learn to reconstruct the foreground. (latent_bg is on 2D grid)
  zwhere_dim: 4  # must be a multiple of 4

  # related to encoding-decoding of glimpses
  glimpse_size: 32  # 28 or 32 only
  zinstance_dim: 32

  # Float values in (0,1) for the IntersectionOverMinimum threshold to be used in non-max-suppression.
  # Lower value lead to a more strict NMS.
  # Do not change them unless you understand what you are doing.
  nms_threshold_train : 0.3 # let it be
  nms_threshold_test : 0.5  # let it be

# parameters describing the input image
input_image:

  # Boolean. Is the background completely zero or not?
  is_zero_background: True

  # Integer. Number of channels of the image
  ch_in: 1

  # Integer. Size, in pixels, of the patches to analyze.
  # It can be smaller than the size of the image.
  # This is useful if a large image is processed by random crops
  patch_size: 64

  # Integer. Maximum number of objects in each patch of square size patch_size x patch_size.
  # A too large value will lead to waste computations.
  # Small value will force the model to group distinct objects together.
  max_objects_per_patch: 10

  # Tuple with the range of the size of the objects (in pixels)
  range_object_size: [10, 35]

  # Tuple with the desired fraction of pixel to be assigned to the foreground objects.
  # The values are in the (0,1) range. If unsure, use vague values.
  # The values are used to guide the model toward the desired segmentation.
  target_fgfraction_min_max: [0.05, 0.15]

  # Tuple with the desired values of Intersection_Over_Union between the inferred and ideal ideal bounding boxes.
  # The values are in the (0,1) range. If unsure, use [0.65, 0.8].
  target_IoU_min_max: [0.65, 0.8]

  # The following parameters determine the DPP-prior which in turns determine the expected number of object in the scene.
  # See our example notebook to see how to choose these parameters.
  # If DPP_learnable_parameters the DPP parameters will be learned during training and the value reported here will
  # be used as initial values
  DPP_learnable_parameters: True
  DPP_length: 5.0
  DPP_weight: 0.3
  target_nobj_av_per_patch_min_max: [2.0, 5.0]

  # The MSE is the pixel mean of: ((x-x_reconstructed) / sigma_mse)^2
  # sigma_mse provides the overall scale it should be chosen so that acceptable reconstructions are of order 1.
  # For example if the input image is in the range (0,1) then sigma_mse should be 0.1
  # We want:
  # target_mse_fg < target_mse_bg = target_mse_for_learnc
  # the strength of the KL regularization will be adjusted to achieve these results
  sigma_mse: 0.1
  target_mse_fg: 1.0
  target_mse_bg: 1.0
  target_mse_for_annealing: 5.0  # when this value is reached I start to anneal down the pretraining



# parameter related to the loss function
# Most likely users should not change these.
loss:

  # parameter related to multi-objective optimization
  multi_objective_optimization: True
  multi_objective_approximation: True
  # approximate_MGDA: False
  # compute_frankwolfe_coeff_frequency: 10

  GMM_observation_model: False
  lambda_fgfraction_max: 25.0
  lambda_nobj_max: 25.0

  lambda_kl_bg_min_max: [0.001, 10.0]
  lambda_kl_fg_min_max: [0.001, 10.0]
  lambda_kl_box_min_max: [0.001, 10.0]
  ideal_box_padding: 3

  # Hyper-parameters which control additional loss terms.
  # Do not change them unless you understand what you are doing
  mask_overlap_penalty_strength: 0.001
  box_overlap_penalty_strength: 0.0

  # Integer. Number of monte-carlo samples used to estimate the KL divergence with the DPP prior
  n_mc_samples: 10

  # Can be "one_detached", "c_detached", "one_attached", "c_attached":
  # Newer version can be: "one", "c", "prob"
  indicator_type: "c"

# parameter related to the optimizer
optimizer:
  # Can be "adam", "SGD" or "RMSprop". In all our experiments we have used Adam.
  type: "adam"

  # Learning rate. Use 3E-4
  lr: 0.001
  lr_geco: 0.001

  # This parameter set the time-window for the moving averages
  beta_moving_averages: 0.999

  # Betas parameters for the Adam optimizer
  betas_adam: [0.9, 0.999]
  betas_geco_adam: [0.9, 0.999]

  # Alpha parameter for the RMSprop optimizer
  alpha_rmsprop: 0.99
  alpha_geco_rmsprop: 0.99


# parameter relative to the learning rate scheduler
scheduler:
  # Boolean. If True the scheduler is active. If False the learning rate is constant during the simulation
  is_active: False

  # Can be "step_LR" or "multistep_LR"
  type: "multistep_LR"

  # Multiplicative factor (less than 1.0) which reduce the learning rate
  gamma: 0.5

  # The "step_LR" scheduler decays the learning rate by gamma every step_size epochs
  step_size: 500

  # The "multistep_LR" scheduler decays the learning rate by gamma once the number of epoch reaches one of the milestones.
  milestones: [200, 400]
