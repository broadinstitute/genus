# neptune project for logging the simulation results
neptune_project: "dalessioluca/genus"

# simulation-level parameters
simulation:
  # can be scratch, resume, pretrained
  type: "scratch"
  BATCH_SIZE: 128
  MAX_EPOCHS: 301
  TEST_FREQUENCY: 1
  CHECKPOINT_FREQUENCY: 10

# architecture parameters
architecture:
  # Integer. Latent dimension for z_instance
  dim_zinstance: 20

  # Integer. Latent dimension for z_where
  dim_zwhere: 4

  # Integer. Latent dimension for z_background
  dim_zbg: 4

  # currently only glimpse_size = 28 will work
  glimpse_size: 28

  # Ingeter parameters which determines how many max-pool are in the unet.
  # Every max-pool lead to a reduction in spatial extension by a factor of 2
  # Usually it is 3,4 or 5
  unet_n_max_pool: 5

  # Integer in the range 0 - unet_n_max_pool (included)
  # It determines the level in the unet at which the bounding boxes are extracted
  # The larger the number the closest to the bottom of the unet the bounding_boxes are extracted
  # Usually it is 1 or 2 less than unet_n_max_pool
  unet_level_boundingboxes: 4

  # This parameter should be a power of 2.
  # At the momnet only 1 = pow(2,0) will work.
  # In future versions we will implement way to deal with very large images
  unet_spatial_downsampling_before_first_max_pool: 1

  # Integer. Number of channels before the first max-pool.
  # Later each max-pool will lead to spatial reduction by a factor of 2
  # and increase in channel number by 2
  unet_ch_before_first_max_pool: 16

  # The number of channels in the feature_map
  # The fature map will be cropped according to bounding boxes to generate z_instance
  unet_ch_feature_map: 16

  # Float values in (0,1) for the IntersectionOverMinimum threshold to be used in non-max-suppression.
  # Lower value lead to a more strict NMS.
  # Do not change them unless you understand what you are doing.
  nms_threshold_train : 0.3
  nms_threshold_test : 0.5

# parameters describing the input image
input_image:
  # Integer. Number of channels of the image
  ch_in: 3

  # Integer. Size, in pixels, of the patches to analyze.
  # This is useful if a large image is processed by random crops
  patch_size: 128

  # Integer. Maximum number of objects in each patch of square size patch_size x patch_size.
  # A too large value will lead to waste computations.
  # Small value will force the model to group distinct objects together.
  max_objects_per_patch: 12

  # Tuple with the range (in pixel) ofor the size of the objects
  range_object_size: [5, 35]

  # Desired value for pixel-averaged Mean Square Error, i.e. MSE = pixel_mean_of (x-x_reconstructed)**2
  # Small value lead to more accurate reconstruction.
  # If the value is too small, the model will ignore all the priors to try to achieve a low MSE.
  # The goal is to segment the scene into objects not to achieve perfect reconstruction.
  # Therefore use a not-to-small value.
  # For example, if the image intensity is the range (0,1) then use 0.1
  target_mse_max: 0.1

  # Tuple with the desired fraction of pixel to be assigned to the foreground objects.
  # The values are in the (0,1) range. If unsure, use vague values.
  # The values are used to guide the model toward the desired segmentation.
  target_fgfraction_min_max: [0.02, 0.10]

  # The following parameters determine the DPP-prior which in turns determine the expected number of object in the scene.
  # At the current moment these parameters are NOT learnable and need to be chosen differently for each dataset.
  # See our example notebook to see how to choose these parameters.
  # Note that this is just the prior, the inferred number of object in each scene will in general deviate from the prior.
  # In future releases, we will make these parameters learnable.
  DPP_learnable: False
  DPP_length: 4.0
  DPP_weight: 1.0
  # In the current release the following parameters are not used
  # DPP_length_min_max: [2.0, 50.0]
  # DPP_weight_min_max: [0.01, 100.0]
  # target_nav_obj_min_max: [2.0, 6.0]

# parameter related to the loss function
loss:
  # Integers. During pre-training we anneal linearly in time a parameter from 1.0 to 0.0.
  # The annealing_times below indicate the duration of the pretraining.
  # Before annealing_times[0] the parameter is 1.0
  # After  annealing_times[1] the parameter is 0.0
  # In between the two, the parameter is decreased linearly.
  # For small-datasets choose values like [50, 100]
  # For large-datasets smaller value will suffice [5, 10]
  annealing_times: [5, 10]

  # The loss functions has many terms whose importance is controlled by the 3 values below.
  # We use an automatic method to adjust these parameters but the user can specify the range of acceptable values.
  # Do not change them unless you understand what you are doing
  lambda_mse_min_max: [0.01, 200.0]
  lambda_ncell_min_max: [0.01, 100]
  lambda_fgfraction_min_max: [0.01, 100]

  # Hyper-parameters which control additional loss terms.
  # Do not change them unless you understand what you are doing
  mask_overlap_penalty_strength: 0.001
  bounding_box_regression_penalty_strength: 0.001
  bounding_box_regression_padding: 3

  # Integer. Number of monte-carlo samples used to estimate the KL divergence with the DPP prior
  n_mc_samples: 100

  # Maximum value for the absolute value of the gradient of logit describing the probability of each foreground object.
  # Do not change them unless you understand what you are doing
  grad_logit_max: 10

# parameter related to the optimizer
optimizer:
  # Can be "adam", "SGD" or "RMSprop". In all our experiments we have used Adam.
  type: "adam"

  # Learning rate.
  lr: 0.001

  # Betas parameters for the Adam optimizer
  betas_adam: [0.9, 0.999]

  # Alpha parameter for the RMSprop optimizer
  alpha_rmsprop: 0.99


# parameter relative to the learning rate scheduler
scheduler:
  # Boolean. If True the scheduler is active. If False the learning rate is constant during the simulation
  is_active: True

  # Can be "step_LR" or "multistep_LR"
  type: "multistep_LR"

  # Multiplicative factor (less than 1.0) which reduce the learning rate
  gamma: 0.1

  # The "step_LR" scheduler decays the learning rate of by gamma every step_size epochs
  step_size: 500

  # The "multistep_LR" scheduler decays the learning rate by gamma once the number of epoch reaches one of the milestones.
  milestones: [50, 100]