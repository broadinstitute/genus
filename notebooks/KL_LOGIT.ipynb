{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST DIFFERENT APPROXIMATIONS FOR KL(logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy\n",
    "from genus.util_ml import Grid_DPP\n",
    "from genus.util_vis import show_batch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def bernoulli_entropy(logit):\n",
    "    p = torch.sigmoid(logit)\n",
    "    one_m_p = torch.sigmoid(-logit)\n",
    "    log_p = F.logsigmoid(logit)\n",
    "    log_one_m_p = F.logsigmoid(-logit)\n",
    "    entropy = - (p * log_p + one_m_p * log_one_m_p)\n",
    "    return entropy\n",
    "\n",
    "def logp_bernoulli(c, logit):\n",
    "    log_p = F.logsigmoid(logit)\n",
    "    log_one_m_p = F.logsigmoid(-logit)\n",
    "    log_prob_bernoulli = (c.detach() * log_p + ~c.detach() * log_one_m_p)\n",
    "    return log_prob_bernoulli\n",
    "\n",
    "def all_configurations(grid_size_x, grid_size_y):\n",
    "    all_c = torch.zeros(2**(grid_size_x*grid_size_y),grid_size_x,grid_size_y).flatten(start_dim=1).numpy()\n",
    "    for n in range(2**(grid_size_x*grid_size_y)):\n",
    "        tmp = bin(n).lstrip(\"0b\").rjust(grid_size_x*grid_size_y, \"0\")\n",
    "        all_c[n] =  numpy.array(list(tmp), dtype=int)\n",
    "    all_c = torch.from_numpy(all_c).view(-1, grid_size_x,grid_size_y).bool()\n",
    "    return all_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size_x, grid_size_y = 3, 3\n",
    "c_all = all_configurations(grid_size_x, grid_size_y).detach()\n",
    "\n",
    "DPP_0 = Grid_DPP(length_scale=10, weight=0.01)\n",
    "DPP_1 = Grid_DPP(length_scale=10, weight=0.01)\n",
    "DPP_2 = Grid_DPP(length_scale=10, weight=0.01)\n",
    "DPP_3 = Grid_DPP(length_scale=10, weight=0.01)\n",
    "DPP_4 = Grid_DPP(length_scale=10, weight=0.01)\n",
    "\n",
    "\n",
    "logit_0 = torch.rand((grid_size_x,grid_size_y)).requires_grad_(False)\n",
    "logit_1 = logit_0.clone().requires_grad_(True)\n",
    "logit_2 = logit_0.clone().requires_grad_(True)\n",
    "logit_3 = logit_0.clone().requires_grad_(True)\n",
    "logit_4 = logit_0.clone().requires_grad_(True)\n",
    "\n",
    "params1 = [logit_1]\n",
    "for name, param in DPP_1.named_parameters():\n",
    "    params1.append(param)\n",
    "    \n",
    "optimizer1 = torch.optim.SGD([{'params': params1, 'lr': 1E-2}])\n",
    "\n",
    "\n",
    "params2 = [logit_2]\n",
    "for name, param in DPP_2.named_parameters():\n",
    "    params2.append(param)\n",
    "    \n",
    "optimizer2 = torch.optim.SGD([{'params': params2, 'lr': 1E-2}])\n",
    "\n",
    "\n",
    "params3 = [logit_3]\n",
    "for name, param in DPP_3.named_parameters():\n",
    "    params3.append(param)\n",
    "    \n",
    "optimizer3 = torch.optim.SGD([{'params': params3, 'lr': 1E-2}])\n",
    "\n",
    "\n",
    "params4 = [logit_4]\n",
    "for name, param in DPP_4.named_parameters():\n",
    "    params4.append(param)\n",
    "    \n",
    "optimizer4 = torch.optim.SGD([{'params': params4, 'lr': 1E-2}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.9330554008483887 1.4890918731689453 1.5636526346206665 tensor(0.) tensor(1.0249)\n",
      "500 0.9256253838539124 1.5320041179656982 1.5609056949615479 tensor(0.) tensor(0.9681)\n",
      "1000 0.9195586442947388 1.528131127357483 1.6069483757019043 tensor(0.) tensor(1.0247)\n",
      "1500 0.9141694903373718 1.5548080205917358 1.6184022426605225 tensor(1.5625) tensor(1.0255)\n",
      "2000 0.9091646671295166 1.4935671091079712 1.6164069175720215 tensor(2.6004) tensor(1.0244)\n",
      "2500 0.9035624265670776 1.4640452861785889 1.5342923402786255 tensor(1.7359) tensor(1.0243)\n",
      "3000 0.8974437713623047 1.4200860261917114 1.5832723379135132 tensor(1.0515) tensor(1.0001)\n",
      "3500 0.891935408115387 1.5104097127914429 1.628333330154419 tensor(0.) tensor(1.0217)\n",
      "4000 0.8872072696685791 1.5085636377334595 1.5596504211425781 tensor(1.2257) tensor(1.0232)\n",
      "4500 0.8825173377990723 1.5245722532272339 1.6188315153121948 tensor(1.3097) tensor(1.0260)\n",
      "5000 0.8782775402069092 1.5575309991836548 1.5757675170898438 tensor(2.6220) tensor(1.0263)\n",
      "5500 0.8745367527008057 1.565212368965149 1.5620617866516113 tensor(1.8650) tensor(1.0265)\n",
      "6000 0.8711485266685486 1.529880404472351 1.5061728954315186 tensor(2.0913) tensor(1.0290)\n",
      "6500 0.8679292798042297 1.572088360786438 1.5752090215682983 tensor(1.0617) tensor(1.0249)\n",
      "7000 0.8644356727600098 1.55681574344635 1.4605518579483032 tensor(1.9272) tensor(1.0308)\n",
      "7500 0.8611370325088501 1.4596890211105347 1.4239270687103271 tensor(3.1477) tensor(0.8612)\n",
      "8000 0.8576915264129639 1.3754162788391113 1.408200740814209 tensor(1.5766) tensor(1.0287)\n",
      "8500 0.85447096824646 1.3980644941329956 1.402518391609192 tensor(0.) tensor(0.9679)\n",
      "9000 0.8514646291732788 1.3977839946746826 1.4919649362564087 tensor(1.2446) tensor(1.0296)\n",
      "9500 0.8484224081039429 1.3814995288848877 1.4644722938537598 tensor(1.2460) tensor(0.9666)\n"
     ]
    }
   ],
   "source": [
    "mc_samples = 4\n",
    "mse = 10.0\n",
    "\n",
    "for epoch in range(10000):\n",
    "    \n",
    "    # Exact\n",
    "    prob_1 = F.sigmoid(logit_1)\n",
    "    c_before_nms = (torch.rand_like(logit_1) < prob_1)\n",
    "    score = c_before_nms + prob_1\n",
    "    c_after_nms = (score == torch.max(score)) * c_before_nms\n",
    "    \n",
    "    entropy_1 = bernoulli_entropy(logit_1).sum()\n",
    "    logp_c_all = DPP_1.log_prob(c_all).detach()\n",
    "    q_c_all = logp_bernoulli(c_all, logit_1).sum(dim=(-1,-2)).exp()\n",
    "    logp_dpp_after = DPP_1.log_prob(c_after_nms).mean()\n",
    "    \n",
    "    loss1 = -entropy_1 - logp_dpp_after -(q_c_all * logp_c_all).sum() - mse * F.sigmoid(logit_1[0,0])\n",
    "    optimizer1.zero_grad()\n",
    "    loss1.backward()\n",
    "    optimizer1.step() \n",
    "    \n",
    "    # reinforce\n",
    "    prob_2 = F.sigmoid(logit_2)\n",
    "    c_before_nms = (torch.rand_like(logit_2.expand(mc_samples,-1,-1)) < prob_2)\n",
    "    score = c_before_nms + prob_2\n",
    "    c_after_nms = (score == torch.max(score)) * c_before_nms\n",
    "    \n",
    "    entropy_2 = bernoulli_entropy(logit_2).sum()\n",
    "    logp_dpp_before = DPP_2.log_prob(c_before_nms)\n",
    "    logp_dpp_after = DPP_2.log_prob(c_after_nms)\n",
    "    logp_ber_before = logp_bernoulli(c_before_nms, logit_2).sum(dim=(-1,-2))\n",
    "    d_tmp = (logp_dpp_before - logp_dpp_before.mean()).abs().mean().detach()\n",
    "    reinforce_2 = logp_ber_before * (logp_dpp_before - logp_dpp_before.mean()).detach()\n",
    "    \n",
    "    loss2 = - entropy_2 - logp_dpp_after.mean() - reinforce_2.mean() - mse * F.sigmoid(logit_2[0,0])\n",
    "    optimizer2.zero_grad()\n",
    "    loss2.backward()\n",
    "    optimizer2.step() \n",
    "    \n",
    "    \n",
    "    # reinforce + importance sampling\n",
    "    prob_3 = F.sigmoid(logit_3)\n",
    "    c_before_nms = (torch.rand_like(logit_3) < prob_3)\n",
    "    score = c_before_nms + prob_3\n",
    "    c_after_nms = (score == torch.max(score)) * c_before_nms\n",
    "    \n",
    "    logit_importance = logit_3.clamp(min=-3.5, max=3.5).detach()\n",
    "    prob_importance = F.sigmoid(logit_importance)\n",
    "    c_importance = (torch.rand_like(logit_importance.expand(mc_samples,-1,-1)) < prob_importance)\n",
    "    \n",
    "    entropy_3 = bernoulli_entropy(logit_3).sum()\n",
    "    logp_dpp_after = DPP_3.log_prob(c_after_nms)\n",
    "    logp_importance = DPP_3.log_prob(c_importance)\n",
    "    logq_importance_star = logp_bernoulli(c_importance, logit_3).sum(dim=(-1,-2))\n",
    "    logq_importance = logp_bernoulli(c_importance, logit_importance.detach()).sum(dim=(-1,-2))\n",
    "    importance_weights = (logq_importance_star - logq_importance).exp().detach()\n",
    "    reinforce_3 = logq_importance_star * (importance_weights*logp_importance - (importance_weights*logp_importance).mean()).detach()\n",
    "        \n",
    "    loss3 = - entropy_3 - logp_dpp_after.mean() - reinforce_3.mean() - mse * F.sigmoid(logit_3[0,0])\n",
    "    optimizer3.zero_grad()\n",
    "    loss3.backward()\n",
    "    optimizer3.step() \n",
    "\n",
    "    \n",
    "    if epoch % 500 == 0:\n",
    "        print(epoch,entropy_1.item(),entropy_2.item(),entropy_3.item(),d_tmp,importance_weights.mean())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIAL PROB\n",
      "nav -> 5.736042499542236\n",
      "tensor([[0.5315, 0.7233, 0.5104],\n",
      "        [0.6619, 0.6944, 0.6782],\n",
      "        [0.7123, 0.5970, 0.6271]])\n",
      "FINAL PROB_1\n",
      "nav -> 1.1985440254211426\n",
      "tensor([[0.9975, 0.0104, 0.0282],\n",
      "        [0.0104, 0.0141, 0.0314],\n",
      "        [0.0283, 0.0313, 0.0470]], grad_fn=<SigmoidBackward>)\n",
      "FINAL PROB_2\n",
      "nav -> 1.3730865716934204\n",
      "tensor([[0.9974, 0.0232, 0.0518],\n",
      "        [0.0217, 0.0185, 0.0548],\n",
      "        [0.0589, 0.0648, 0.0821]], grad_fn=<SigmoidBackward>)\n",
      "FINAL PROB_3\n",
      "nav -> 1.3659470081329346\n",
      "tensor([[0.9885, 0.0237, 0.0553],\n",
      "        [0.0286, 0.0348, 0.0582],\n",
      "        [0.0531, 0.0597, 0.0639]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(\"INITIAL PROB\")\n",
    "print(\"nav ->\",F.sigmoid(logit_0).sum().item())\n",
    "print(F.sigmoid(logit_0))\n",
    "\n",
    "print(\"FINAL PROB_1\")\n",
    "print(\"nav ->\",F.sigmoid(logit_1).sum().item())\n",
    "print(F.sigmoid(logit_1))\n",
    "\n",
    "print(\"FINAL PROB_2\")\n",
    "print(\"nav ->\",F.sigmoid(logit_2).sum().item())\n",
    "print(F.sigmoid(logit_2))\n",
    "\n",
    "print(\"FINAL PROB_3\")\n",
    "print(\"nav ->\",F.sigmoid(logit_3).sum().item())\n",
    "print(F.sigmoid(logit_3))\n",
    "\n",
    "# print(\"FINAL PROB_4\")\n",
    "# print(\"nav ->\",F.sigmoid(logit_4).sum().item())\n",
    "# print(F.sigmoid(logit_4))\n",
    "# \n",
    "# print(\"FINAL DPP_1\")\n",
    "# print(DPP_1.fingerprint[:2])\n",
    "# final_dpp_sample_1 = DPP_1.sample(size=logit_1.expand(10000,-1,-1).size())\n",
    "# nav_final_1 = final_dpp_sample_1.sum(dim=(-1,-2)).float().mean()\n",
    "# print(\"nav ->\",nav_final_1)\n",
    "# print(final_dpp_sample_1.float().mean(dim=0))\n",
    "# \n",
    "# print(\"FINAL DPP_2\")\n",
    "# print(DPP_2.fingerprint[:2])\n",
    "# final_dpp_sample_2 = DPP_2.sample(size=logit_2.expand(10000,-1,-1).size())\n",
    "# nav_final_2 = final_dpp_sample_2.sum(dim=(-1,-2)).float().mean()\n",
    "# print(\"nav ->\",nav_final_2)\n",
    "# print(final_dpp_sample_2.float().mean(dim=0))\n",
    "# \n",
    "# print(\"FINAL DPP_3\")\n",
    "# print(DPP_3.fingerprint[:2])\n",
    "# final_dpp_sample_3 = DPP_3.sample(size=logit_3.expand(10000,-1,-1).size())\n",
    "# nav_final_3 = final_dpp_sample_3.sum(dim=(-1,-2)).float().mean()\n",
    "# print(\"nav ->\",nav_final_3)\n",
    "# print(final_dpp_sample_3.float().mean(dim=0))\n",
    "# \n",
    "# print(\"FINAL DPP_4\")\n",
    "# print(DPP_4.fingerprint[:2])\n",
    "# final_dpp_sample_4 = DPP_4.sample(size=logit_4.expand(10000,-1,-1).size())\n",
    "# nav_final_4 = final_dpp_sample_4.sum(dim=(-1,-2)).float().mean()\n",
    "# print(\"nav ->\",nav_final_4)\n",
    "# print(final_dpp_sample_4.float().mean(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.9691, -4.5568, -3.5381],\n",
      "        [-4.5598, -4.2485, -3.4295],\n",
      "        [-3.5352, -3.4310, -3.0103]], requires_grad=True)\n",
      "tensor([[ 5.9356, -3.7393, -2.9065],\n",
      "        [-3.8066, -3.9717, -2.8483],\n",
      "        [-2.7721, -2.6701, -2.4147]], requires_grad=True)\n",
      "tensor([[ 4.4498, -3.7165, -2.8374],\n",
      "        [-3.5242, -3.3233, -2.7838],\n",
      "        [-2.8806, -2.7561, -2.6839]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(logit_1)\n",
    "print(logit_2)\n",
    "print(logit_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0087,  0.0048,  0.0019],\n",
      "        [ 0.0047,  0.0037,  0.0017],\n",
      "        [ 0.0020,  0.0017,  0.0013]])\n",
      "tensor([[-0.0107, -0.0848, -0.1427],\n",
      "        [-0.0809, -0.0721,  0.2563],\n",
      "        [-0.1534,  0.2421, -0.1817]])\n",
      "tensor([[-0.0634, -0.0861, -0.1482],\n",
      "        [-0.0980, -0.1115, -0.1525],\n",
      "        [-0.1448, -0.1547,  0.3806]])\n"
     ]
    }
   ],
   "source": [
    "print(logit_1.grad)\n",
    "print(logit_2.grad)\n",
    "print(logit_3.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "neptune": {
   "notebookId": "043e4b2a-815e-4e6f-8132-e2cdd0a0e054"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
