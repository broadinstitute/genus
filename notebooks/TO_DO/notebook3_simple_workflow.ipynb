{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrate a simple workflow. Run notebook \"notebook1_create_synthetic_dataset.ipynb\" first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cellsegmenter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8acbdd088155>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcellsegmenter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#from cellsegmenter import SpecialDataSet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcellsegmenter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil_ml\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSpecialDataSet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcellsegmenter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCompositionalVae\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcellsegmenter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cellsegmenter'"
     ]
    }
   ],
   "source": [
    "import cellsegmenter\n",
    "#from cellsegmenter import SpecialDataSet\n",
    "from cellsegmenter.util_ml import SpecialDataSet\n",
    "from cellsegmenter import CompositionalVae\n",
    "from cellsegmenter.model import *\n",
    "from cellsegmenter.util import load_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an instance of the CompisitionalVae with the default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neptune_project': 'dalessioluca/MNIST',\n",
       " 'simulation': {'__comment': 'there are 3 types of runs: scratch, resume, pretrained',\n",
       "  'type': 'scratch',\n",
       "  'MAX_EPOCHS': 501,\n",
       "  'TEST_FREQUENCY': 50,\n",
       "  'CHECKPOINT_FREQUENCY': 10,\n",
       "  'batch_size': 128},\n",
       " 'architecture': {'__comment': 'architecture parameters, level_zwhere_logit is between 0 and n_max_pool included',\n",
       "  'dim_zinstance': 20,\n",
       "  'dim_zwhere': 4,\n",
       "  'dim_zbg': 20,\n",
       "  'cropped_size': 28,\n",
       "  'n_max_pool_unet': 4,\n",
       "  'level_zwherelogit_unet': 2,\n",
       "  'n_ch_img': 1,\n",
       "  'n_ch_output_features': 16,\n",
       "  'n_ch_after_preprocessing': 16,\n",
       "  'downsampling_factor_during_preprocessing': 1,\n",
       "  'nms_threshold_train': 0.3,\n",
       "  'nms_threshold_test': 0.5},\n",
       " 'input_image': {'__comment': 'parameters describing the input images',\n",
       "  'k_objects_max': 10,\n",
       "  'size_object_range': [15, 50],\n",
       "  'size_raw_image': 80,\n",
       "  'similarity_DPP_l': 10.0,\n",
       "  'similarity_DPP_w': 1.0,\n",
       "  'similarity_DPP_l_min_max': [2.0, 50.0],\n",
       "  'similarity_DPP_w_min_max': [0.1, 10.0]},\n",
       " 'shortcut_prob_corr_factor': {'__comment': 'parameters for the shortcut for porb_corr_factor',\n",
       "  'values': [0.4, 0.0],\n",
       "  'times': [50, 150]},\n",
       " 'loss': {'__comment': 'if active=false use ELBO, else use GECO with Log-Likelihood threshold = n_pixels * n_channel * threshold',\n",
       "  'geco_mse_target': 0.1,\n",
       "  'geco_fgfraction_target': [0.02, 0.1],\n",
       "  'geco_ncell_target': [0.1, 10.0],\n",
       "  'geco_lambda_mse_max': 10,\n",
       "  'geco_lambda_ncell_max': 20,\n",
       "  'geco_lambda_fgfraction_max': 20,\n",
       "  'mask_overlap_penalty_strength': 0.001,\n",
       "  'bounding_box_regression_penalty_strength': 0.001,\n",
       "  'bounding_box_regression_padding': 4},\n",
       " 'optimizer': {'__comment': 'which optimizer to use',\n",
       "  'type': 'adam',\n",
       "  'base_lr': 0.001,\n",
       "  'betas': [0.9, 0.999],\n",
       "  'base_lr_similarity': 0.01,\n",
       "  'betas_similarity': [0.9, 0.999],\n",
       "  'base_lr_geco': 0.001,\n",
       "  'betas_geco': [0.9, 0.999],\n",
       "  'weight_decay': 0.0,\n",
       "  'eps': 1e-08,\n",
       "  'scheduler_is_active': False,\n",
       "  'scheduler_type': 'step_LR',\n",
       "  'scheduler_step_size': 500,\n",
       "  'scheduler_gamma': 0.75}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = CompositionalVae.default_params()\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = CompositionalVae(params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the dataset created by \"notebook1_create_synthetic_dataset.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_obj' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0a3b557cd92e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimg_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseg_mask_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data_train.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m train_loader = SpecialDataSet(img=img_train,\n\u001b[1;32m      5\u001b[0m                               \u001b[0mroi_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_obj' is not defined"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE=64\n",
    "img_train, seg_mask_train, count_train = load_obj(\"./data_train.pt\")\n",
    "\n",
    "train_loader = SpecialDataSet(img=img_train,\n",
    "                              roi_mask=None,\n",
    "                              seg_mask=seg_mask_train,\n",
    "                              labels=count_train,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              drop_last=True,\n",
    "                              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class SpecialDataSet in module cellsegmenter.util_ml:\n",
      "\n",
      "class SpecialDataSet(builtins.object)\n",
      " |  SpecialDataSet(x: torch.Tensor, x_roi: Union[torch.Tensor, NoneType] = None, y: Union[torch.Tensor, NoneType] = None, labels: Union[torch.Tensor, NoneType] = None, data_augmentation: Union[cellsegmenter.util_ml.ConditionalRandomCrop, NoneType] = None, store_in_cuda: bool = False, batch_size: int = 4, drop_last: bool = False, shuffle: bool = True)\n",
      " |  \n",
      " |  Dataset and dataloader combined into single class with extra features.\n",
      " |  \n",
      " |  Note:\n",
      " |      This class is useful for small datasets which fit into CPU or GPU memory.\n",
      " |  \n",
      " |  Note:\n",
      " |      It can be used in combination with :class:`ConditionalRandomCrop` to create a\n",
      " |      dataset out of a single large image.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, index: torch.Tensor)\n",
      " |      :meta public:\n",
      " |      \n",
      " |      Args:\n",
      " |          torch.Tensor of type long specifying the index of the images to load. Each entry should be between\n",
      " |              0 and self.__len__(). Repeated index are allowed.\n",
      " |      \n",
      " |      Returns:\n",
      " |           A tuple with x, y, labels, index. Where x,y are the images and labels and index are scalars.\n",
      " |           The size of the output is determined by the length of :attr:`index`.\n",
      " |  \n",
      " |  __init__(self, x: torch.Tensor, x_roi: Union[torch.Tensor, NoneType] = None, y: Union[torch.Tensor, NoneType] = None, labels: Union[torch.Tensor, NoneType] = None, data_augmentation: Union[cellsegmenter.util_ml.ConditionalRandomCrop, NoneType] = None, store_in_cuda: bool = False, batch_size: int = 4, drop_last: bool = False, shuffle: bool = True)\n",
      " |      Args:\n",
      " |          x: the underlying image or batch of images which composed the dataset of shape :math:`(B,C,W,H)`\n",
      " |          x_roi: boolean tensor indicating the Region of Interest (ROI) of shape :math:`(B,1,W,H)`\n",
      " |          y: integer tensor with the pixel-level labels of shape :math:`(B,1,W,H)`\n",
      " |          labels: integer tensor with the image-level labels of shape :math:`(B)`\n",
      " |          data_augmentation: if specified both x and y are processed by this function on the fly every\n",
      " |              time the :meth:`__getitem__` is called.\n",
      " |          store_in_cuda: if true the dataset is stored into GPU memory, if false the dataset is stored in CPU memory.\n",
      " |              Either way the dataset is stored in memory for faster processing.\n",
      " |              This means that this class is useful for small dataset.\n",
      " |          batch_size: default value of the size of the minibatch which will be loaded every\n",
      " |              time :meth:`__iter__` is called\n",
      " |          drop_last: if true the last minibatch in each epoch will be dropped if the dataset size is not\n",
      " |              an exact multiple of the minibatch size.\n",
      " |          shuffle: it true the order of the element is randomized. If false the element are loaded in exactly the same\n",
      " |              order at each epoch.\n",
      " |      \n",
      " |      Note:\n",
      " |          The underlying data is all stored in memory for faster processing\n",
      " |          (either CPU or GPU depending on :attr:`store_in_cuda`). This means that this class is usefull only for\n",
      " |          relatively small datasets.\n",
      " |      \n",
      " |      Note:\n",
      " |          If :attr:`data_augmentation` is equal to :class:`ConditionalRandomCrop` the dataloader returns different\n",
      " |          random crops at every call thus implementing data augmentation.\n",
      " |      \n",
      " |      Examples:\n",
      " |          >>> # Create the random crops once and add them in the test_dataloader\n",
      " |          >>> crop_size, test_dataset_size, minibatch_size = 80, 128, 64\n",
      " |          >>> conditional_crop_test = ConditionalRandomCrop(desired_w=crop_size,\n",
      " |          >>>                                               desired_h=crop_size,\n",
      " |          >>>                                               min_roi_fraction=0.1,\n",
      " |          >>>                                               n_crops_per_image=test_dataset_size)\n",
      " |          >>> test_data = conditional_crop_test.crop(img=one_large_image, roi_mask=one_large_image_roi_mask)\n",
      " |          >>> # save the small test_dataset on GPU memory\n",
      " |          >>> test_loader = SpecialDataSet(img=test_data,\n",
      " |          >>>                              store_in_cuda=True,\n",
      " |          >>>                              shuffle=False,\n",
      " |          >>>                              drop_last=False)\n",
      " |          >>> # Create random crops on the fly (i.e. data-augmentation) to create a constantly changing train_dataset\n",
      " |          >>> conditional_crop_train = ConditionalRandomCrop(desired_w=crop_size,\n",
      " |          >>>                                                desired_h=crop_size,\n",
      " |          >>>                                                min_roi_fraction=0.9,\n",
      " |          >>>                                                n_crops_per_image=minibatch_size)\n",
      " |          >>> # save the large image on CPU memory and every time draw random crops from it.\n",
      " |          >>> train_loader = SpecialDataSet(img=one_large_image,\n",
      " |          >>>                               roi_mask=one_large_image_roi_mask,\n",
      " |          >>>                               store_in_cuda=False,\n",
      " |          >>>                               shuffle=True,\n",
      " |          >>>                               data_augmentation=conditional_crop_train,\n",
      " |          >>>                               batch_size=minibatch_size)\n",
      " |  \n",
      " |  __iter__(self, batch_size=None, drop_last=None, shuffle=None)\n",
      " |  \n",
      " |  __len__(self)\n",
      " |  \n",
      " |  check_batch(self, batch_size: int = 8)\n",
      " |      Print some information about the dataset and load a random batch of images\n",
      " |      \n",
      " |      Args:\n",
      " |          batch_size: the size of the batxh of images to load\n",
      " |  \n",
      " |  load(self, batch_size: Union[int, NoneType] = None, index: Union[torch.Tensor, NoneType] = None)\n",
      " |      Load a batch of images\n",
      " |      \n",
      " |      Args:\n",
      " |          batch_size: number of images to randomly load from the dataset\n",
      " |          index: torch.Tensor of type long specifying the index of the images to load. Each entry should be between\n",
      " |              0 and self.__len__(). Repeated index are allowed. The batch_size will be equal to the length of\n",
      " |              :attr:`index`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A batch of images\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(SpecialDataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data_test.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-4cfcec38f950>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimg_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseg_mask_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data_train.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimg_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseg_mask_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data_test.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"simulation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"batch_size\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m train_loader = SpecialDataSet(img=img_train,\n",
      "\u001b[0;32m~/REPOS/cellsegmenter/src/cellsegmenter/util.py\u001b[0m in \u001b[0;36mload_obj\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdill\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data_test.pt'"
     ]
    }
   ],
   "source": [
    "img_train, seg_mask_train, count_train = load_obj(\"./data_train.pt\")\n",
    "img_test, seg_mask_test, count_test = load_obj(\"./data_test.pt\")\n",
    "BATCH_SIZE = params[\"simulation\"][\"batch_size\"]\n",
    "\n",
    "train_loader = SpecialDataSet(img=img_train,\n",
    "                              roi_mask=None,\n",
    "                              seg_mask=seg_mask_train,\n",
    "                              labels=count_train,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              drop_last=False,\n",
    "                              shuffle=True)\n",
    "\n",
    "train_batch_example_fig = train_loader.check_batch()\n",
    "log_img_only(name=\"train_batch_example\", fig=train_batch_example_fig, experiment=exp)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU GB after train_loader ->\", torch.cuda.memory_allocated()/1E9)\n",
    "\n",
    "test_loader = SpecialDataSet(img=img_test,\n",
    "                             roi_mask=None,\n",
    "                             seg_mask=seg_mask_test,\n",
    "                             labels=count_test,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             drop_last=False,\n",
    "                             shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'size_object_range'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-fa354e7f8ec2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_json_as_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./ML_parameters.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCompositionalVae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/REPOS/cellsegmenter/src/cellsegmenter/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Instantiate all the modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference_and_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInferenceAndGeneration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/REPOS/cellsegmenter/src/cellsegmenter/model_parts.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_image\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"size_object_range\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_image\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"size_object_range\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcropped_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"architecture\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cropped_size\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'size_object_range'"
     ]
    }
   ],
   "source": [
    "params = load_json_as_dict(\"./ML_parameters.json\")\n",
    "vae = CompositionalVae(params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = cs.load_json_as_dict(\"./ML_parameters.json\")\n",
    "vae = CompositionalVae(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.encoders_decoders import *\n",
    "import torch\n",
    "\n",
    "from src.vae_model import *\n",
    "from src.utilities_ml import process_one_epoch\n",
    "from src.utilities_visualization import show_batch\n",
    "import torch.nn.functional as F\n",
    "from src.utilities_ml import ConditionalRandomCrop, SpecialDataSet, process_one_epoch\n",
    "\n",
    "\n",
    "# Check versions\n",
    "from platform import python_version\n",
    "print(\"python_version() ---> \", python_version())\n",
    "print(\"torch.__version__ --> \", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = load_json_as_dict(\"./ML_parameters.json\")\n",
    "vae = CompositionalVae(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
