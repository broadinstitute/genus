{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KL between Bernoulli Posterior and DPP prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to compute the KL divergence between a DPP prior and a Bernoulli posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U neptune-notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy\n",
    "from genus.util_ml import Grid_DPP, compute_logp_bernoulli, compute_entropy_bernoulli\n",
    "from genus.util_vis import show_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simplified_model(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 length_scale: float=5.0,\n",
    "                 weight: float=0.2,\n",
    "                 n_mc_samples: int=10):\n",
    "        super().__init__()\n",
    "        B, W, H = 8, 10, 10\n",
    "        self.length_scale = length_scale\n",
    "        self.weight = weight\n",
    "        self.n_mc_samples = n_mc_samples\n",
    "        self.grid_dpp = Grid_DPP(length_scale=length_scale,\n",
    "                                 weight=weight,\n",
    "                                 learnable_params=False)\n",
    "        self.logit = torch.nn.Parameter(data=torch.randn((B,1,W,H), dtype=torch.float), requires_grad=True)\n",
    "        \n",
    "    def sample_dpp_prior(self):\n",
    "        return self.grid_dpp.sample(size=self.logit.size())\n",
    "        \n",
    "    \n",
    "    def forward(self):\n",
    "\n",
    "        # This term can be compute analytically\n",
    "        entropy_ber = compute_entropy_bernoulli(logit=self.logit).sum(dim=(-1, -2, -3)).mean()\n",
    "  \n",
    "        # Use the Bernoulli distribution at Temperature = 1 to evaluate the KL divergence\n",
    "        prob_b1wh = torch.sigmoid(self.logit)\n",
    "        prob_expanded_nb1wh = prob_b1wh.expand([self.n_mc_samples,-1,-1,-1,-1])  # keep: batch,ch,w,h\n",
    "        c_mcsamples_nb1wh = (torch.rand_like(prob_expanded_nb1wh) < prob_expanded_nb1wh)\n",
    "        logp_ber_nb = compute_logp_bernoulli(c=c_mcsamples_nb1wh.detach(), logit=self.logit).sum(dim=(-1, -2, -3))\n",
    "        with torch.no_grad():\n",
    "            logp_dpp_nb = self.grid_dpp.log_prob(value=c_mcsamples_nb1wh.squeeze(-3).detach())\n",
    "            baseline_b = logp_dpp_nb.mean(dim=0) \n",
    "            d_nb = (logp_dpp_nb - baseline_b)\n",
    "        reinforce_ber = (logp_ber_nb * d_nb.detach()).mean()\n",
    "        logit_kl_av = - entropy_ber - reinforce_ber\n",
    "        \n",
    "        loss = logit_kl_av\n",
    "        c_sample = (torch.rand_like(self.logit) < torch.sigmoid(self.logit))\n",
    "        return loss, c_sample, reinforce_ber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ui.neptune.ai/cellarium/genus/e/GEN-158\n"
     ]
    }
   ],
   "source": [
    "import neptune.new as neptune\n",
    "\n",
    "exp = neptune.init(project='cellarium/genus')\n",
    "                                    \n",
    "model = simplified_model(length_scale=5.0, \n",
    "                         weight=0.2, \n",
    "                         n_mc_samples=10)\n",
    "optimizer = torch.optim.Adam([{'params': model.parameters(), 'lr': 0.001}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log info about the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp[\"config/length_scale\"].log(model.length_scale)\n",
    "exp[\"config/weight\"].log(model.weight)\n",
    "exp[\"config/n_mc_samples\"].log(model.n_mc_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log info about the prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_grid = model.sample_dpp_prior()\n",
    "n_av = model.grid_dpp.n_mean\n",
    "n_std = model.grid_dpp.n_stddev\n",
    "\n",
    "exp[\"prior/n_cell_av\"].log(n_av)\n",
    "exp[\"prior/n_cell_p1sigma\"].log(n_av+n_std)\n",
    "exp[\"prior/n_cell_m1sigma\"].log(n_av-n_std)\n",
    "\n",
    "show_batch(model.logit, pad_value=1.0, n_padding=2, normalize_range=(-3.0, 3.0), \n",
    "               normalize=True, experiment=exp, neptune_name=\"prior/logit_raw\", title=\"PRIOR\")\n",
    "show_batch(c_grid.float(), pad_value=0.5, n_padding=2, experiment=exp, neptune_name=\"prior/c_grid\", title=\"PRIOR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(-69.7418, grad_fn=<SubBackward0>)\n",
      "100 tensor(-79.7570, grad_fn=<SubBackward0>)\n",
      "200 tensor(-67.4798, grad_fn=<SubBackward0>)\n",
      "300 tensor(-77.9402, grad_fn=<SubBackward0>)\n",
      "400 tensor(-55.4912, grad_fn=<SubBackward0>)\n",
      "500 tensor(-54.4585, grad_fn=<SubBackward0>)\n",
      "600 tensor(-63.3943, grad_fn=<SubBackward0>)\n",
      "700 tensor(-43.3412, grad_fn=<SubBackward0>)\n",
      "800 tensor(-65.5327, grad_fn=<SubBackward0>)\n",
      "900 tensor(-63.0782, grad_fn=<SubBackward0>)\n",
      "1000 tensor(-62.4086, grad_fn=<SubBackward0>)\n",
      "1100 tensor(-48.8099, grad_fn=<SubBackward0>)\n",
      "1200 tensor(-52.2643, grad_fn=<SubBackward0>)\n",
      "1300 tensor(-65.8219, grad_fn=<SubBackward0>)\n",
      "1400 tensor(-80.7400, grad_fn=<SubBackward0>)\n",
      "1500 tensor(-65.6604, grad_fn=<SubBackward0>)\n",
      "1600 tensor(-58.4161, grad_fn=<SubBackward0>)\n",
      "1700 tensor(-56.2502, grad_fn=<SubBackward0>)\n",
      "1800 tensor(-59.8957, grad_fn=<SubBackward0>)\n",
      "1900 tensor(-74.6978, grad_fn=<SubBackward0>)\n",
      "2000 tensor(-49.9101, grad_fn=<SubBackward0>)\n",
      "2100 tensor(-74.2388, grad_fn=<SubBackward0>)\n",
      "2200 tensor(-68.8985, grad_fn=<SubBackward0>)\n",
      "2300 tensor(-67.3048, grad_fn=<SubBackward0>)\n",
      "2400 tensor(-66.1678, grad_fn=<SubBackward0>)\n",
      "2500 tensor(-73.0515, grad_fn=<SubBackward0>)\n",
      "2600 tensor(-62.1220, grad_fn=<SubBackward0>)\n",
      "2700 tensor(-52.0486, grad_fn=<SubBackward0>)\n",
      "2800 tensor(-56.1442, grad_fn=<SubBackward0>)\n",
      "2900 tensor(-57.6341, grad_fn=<SubBackward0>)\n",
      "3000 tensor(-72.0307, grad_fn=<SubBackward0>)\n",
      "3100 tensor(-65.0920, grad_fn=<SubBackward0>)\n",
      "3200 tensor(-61.9950, grad_fn=<SubBackward0>)\n",
      "3300 tensor(-51.8676, grad_fn=<SubBackward0>)\n",
      "3400 tensor(-61.5147, grad_fn=<SubBackward0>)\n",
      "3500 tensor(-67.4290, grad_fn=<SubBackward0>)\n",
      "3600 tensor(-67.9286, grad_fn=<SubBackward0>)\n",
      "3700 tensor(-55.6197, grad_fn=<SubBackward0>)\n",
      "3800 tensor(-53.3849, grad_fn=<SubBackward0>)\n",
      "3900 tensor(-63.4710, grad_fn=<SubBackward0>)\n",
      "4000 tensor(-51.3564, grad_fn=<SubBackward0>)\n",
      "4100 tensor(-65.7348, grad_fn=<SubBackward0>)\n",
      "4200 tensor(-58.8384, grad_fn=<SubBackward0>)\n",
      "4300 tensor(-56.8231, grad_fn=<SubBackward0>)\n",
      "4400 tensor(-48.5913, grad_fn=<SubBackward0>)\n",
      "4500 tensor(-80.0671, grad_fn=<SubBackward0>)\n",
      "4600 tensor(-57.5033, grad_fn=<SubBackward0>)\n",
      "4700 tensor(-71.4608, grad_fn=<SubBackward0>)\n",
      "4800 tensor(-55.8288, grad_fn=<SubBackward0>)\n",
      "4900 tensor(-60.4907, grad_fn=<SubBackward0>)\n",
      "5000 tensor(-65.8639, grad_fn=<SubBackward0>)\n",
      "5100 tensor(-50.0434, grad_fn=<SubBackward0>)\n",
      "5200 tensor(-68.5455, grad_fn=<SubBackward0>)\n",
      "5300 tensor(-48.5048, grad_fn=<SubBackward0>)\n",
      "5400 tensor(-57.3309, grad_fn=<SubBackward0>)\n",
      "5500 tensor(-48.4995, grad_fn=<SubBackward0>)\n",
      "5600 tensor(-55.8452, grad_fn=<SubBackward0>)\n",
      "5700 tensor(-77.6063, grad_fn=<SubBackward0>)\n",
      "5800 tensor(-51.4686, grad_fn=<SubBackward0>)\n",
      "5900 tensor(-67.4344, grad_fn=<SubBackward0>)\n",
      "6000 tensor(-52.8605, grad_fn=<SubBackward0>)\n",
      "6100 tensor(-55.9537, grad_fn=<SubBackward0>)\n",
      "6200 tensor(-61.0113, grad_fn=<SubBackward0>)\n",
      "6300 tensor(-61.7481, grad_fn=<SubBackward0>)\n",
      "6400 tensor(-54.2001, grad_fn=<SubBackward0>)\n",
      "6500 tensor(-54.0809, grad_fn=<SubBackward0>)\n",
      "6600 tensor(-49.0606, grad_fn=<SubBackward0>)\n",
      "6700 tensor(-63.0015, grad_fn=<SubBackward0>)\n",
      "6800 tensor(-51.8045, grad_fn=<SubBackward0>)\n",
      "6900 tensor(-51.1752, grad_fn=<SubBackward0>)\n",
      "7000 tensor(-60.0520, grad_fn=<SubBackward0>)\n",
      "7100 tensor(-56.3848, grad_fn=<SubBackward0>)\n",
      "7200 tensor(-48.9551, grad_fn=<SubBackward0>)\n",
      "7300 tensor(-55.7710, grad_fn=<SubBackward0>)\n",
      "7400 tensor(-71.8094, grad_fn=<SubBackward0>)\n",
      "7500 tensor(-70.6422, grad_fn=<SubBackward0>)\n",
      "7600 tensor(-54.9157, grad_fn=<SubBackward0>)\n",
      "7700 tensor(-54.1713, grad_fn=<SubBackward0>)\n",
      "7800 tensor(-53.5488, grad_fn=<SubBackward0>)\n",
      "7900 tensor(-54.3937, grad_fn=<SubBackward0>)\n",
      "8000 tensor(-61.5658, grad_fn=<SubBackward0>)\n",
      "8100 tensor(-69.4615, grad_fn=<SubBackward0>)\n",
      "8200 tensor(-57.6068, grad_fn=<SubBackward0>)\n",
      "8300 tensor(-49.1155, grad_fn=<SubBackward0>)\n",
      "8400 tensor(-52.9916, grad_fn=<SubBackward0>)\n",
      "8500 tensor(-58.6293, grad_fn=<SubBackward0>)\n",
      "8600 tensor(-49.9056, grad_fn=<SubBackward0>)\n",
      "8700 tensor(-52.8648, grad_fn=<SubBackward0>)\n",
      "8800 tensor(-55.5822, grad_fn=<SubBackward0>)\n",
      "8900 tensor(-67.6600, grad_fn=<SubBackward0>)\n",
      "9000 tensor(-75.5351, grad_fn=<SubBackward0>)\n",
      "9100 tensor(-54.1548, grad_fn=<SubBackward0>)\n",
      "9200 tensor(-62.1681, grad_fn=<SubBackward0>)\n",
      "9300 tensor(-56.8226, grad_fn=<SubBackward0>)\n",
      "9400 tensor(-65.7626, grad_fn=<SubBackward0>)\n",
      "9500 tensor(-57.6787, grad_fn=<SubBackward0>)\n",
      "9600 tensor(-80.6590, grad_fn=<SubBackward0>)\n",
      "9700 tensor(-54.3512, grad_fn=<SubBackward0>)\n",
      "9800 tensor(-54.8709, grad_fn=<SubBackward0>)\n",
      "9900 tensor(-60.6997, grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10000):\n",
    "    loss, c_grid, delta = model.forward()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()  # do back_prop and compute all the gradients\n",
    "    optimizer.step()  # update the parameters    \n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        n_cell_b = c_grid.sum(dim=(-1,-2,-3)).float()\n",
    "        n_cell_std, n_cell_av = torch.std_mean(n_cell_b)\n",
    "        n_cell_p1sigma = (n_cell_av+n_cell_std).detach().item()\n",
    "        n_cell_m1sigma = (n_cell_av-n_cell_std).detach().item()\n",
    "        \n",
    "        n_cell_min = n_cell_b.min().detach().item()\n",
    "        n_cell_max = n_cell_b.max().detach().item()\n",
    "        exp[\"posterior/n_cell_av\"].log(n_cell_av)\n",
    "        exp[\"posterior/n_cell_min\"].log(n_cell_min)\n",
    "        exp[\"posterior/n_cell_max\"].log(n_cell_max)        \n",
    "        exp[\"posterior/n_cell_p1sigma\"].log(n_cell_p1sigma)\n",
    "        exp[\"posterior/n_cell_m1sigma\"].log(n_cell_m1sigma)\n",
    "\n",
    "        logit_min = torch.min(model.logit).item()\n",
    "        logit_mean = torch.mean(model.logit).item()\n",
    "        logit_max = torch.max(model.logit).item()\n",
    "        exp[\"posterior/logit_min\"].log(logit_min)\n",
    "        exp[\"posterior/logit_mean\"].log(logit_mean)\n",
    "        exp[\"posterior/logit_max\"].log(logit_max)\n",
    "        \n",
    "        exp[\"delta\"].log(delta.detach().item())\n",
    "        \n",
    "        show_batch(c_grid.float(), pad_value=0.5, n_padding=2, experiment=exp, neptune_name=\"posterior/c_grid\", \n",
    "                   title=\"epoch=\"+str(epoch))\n",
    "        show_batch(model.logit, pad_value=1.0, n_padding=2, normalize_range=(-5.0, 5.0), \n",
    "               normalize=True, experiment=exp, neptune_name=\"posterior/logit_raw\", title=\"epoch=\"+str(epoch))\n",
    "        \n",
    "        print(epoch, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze final value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit_min -3.9467544555664062\n",
      "logit_mean -3.2642464637756348\n",
      "logit_max -2.4515037536621094\n"
     ]
    }
   ],
   "source": [
    "logit_final = model.logit.clone()\n",
    "\n",
    "print(\"logit_min\", logit_final.min().cpu().detach().item())\n",
    "print(\"logit_mean\", logit_final.mean().cpu().detach().item())\n",
    "print(\"logit_max\", logit_final.max().cpu().detach().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_final_prob = n_av / torch.numel(logit_final[-2:])\n",
    "expected_final_logit = torch.log(expected_final_prob) - torch.log(1-expected_final_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-3.9604)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-15:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/neptune/new/internal/backends/utils.py\", line 51, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/neptune/new/internal/backends/hosted_neptune_backend.py\", line 229, in ping_run\n",
      "    self.leaderboard_client.api.ping(experimentId=str(run_uuid)).response().result\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/bravado/http_future.py\", line 239, in response\n",
      "    six.reraise(*sys.exc_info())\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/six.py\", line 703, in reraise\n",
      "    raise value\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/bravado/http_future.py\", line 200, in response\n",
      "    swagger_result = self._get_swagger_result(incoming_response)\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/bravado/http_future.py\", line 124, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/bravado/http_future.py\", line 300, in _get_swagger_result\n",
      "    unmarshal_response(\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/bravado/http_future.py\", line 334, in unmarshal_response\n",
      "    raise_on_unexpected(incoming_response)\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/bravado/http_future.py\", line 408, in raise_on_unexpected\n",
      "    raise make_http_exception(response=http_response)\n",
      "bravado.exception.HTTPServiceUnavailable: 503 : {\"errorType\":\"SERVICE_UNAVAILABLE\",\"code\":503,\"title\":\"Service is not available. Try again later.\"}\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/neptune/new/internal/threading/daemon.py\", line 46, in run\n",
      "    self.work()\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/neptune/new/internal/utils/ping_background_job.py\", line 62, in work\n",
      "    self._run.ping()\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/neptune/new/run.py\", line 94, in ping\n",
      "    self._backend.ping_run(self._uuid)\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/neptune/new/internal/backends/utils.py\", line 92, in wrapper\n",
      "    raise NeptuneConnectionLostException() from last_exception\n",
      "neptune.new.exceptions.NeptuneConnectionLostException: \n",
      "\u001b[95m\n",
      "----NeptuneConnectionLostException---------------------------------------------------------\n",
      "\u001b[0m\n",
      "A connection to the Neptune server was lost.\n",
      "If you are using asynchronous (default) connection mode Neptune automatically switched to an offline mode and your data is being stored safely on the disk.\n",
      "You can upload it later using Neptune Command Line Interface:\n",
      "    \u001b[95mneptune sync -p workspace_name/project_name\u001b[0m\n",
      "\n",
      "What should I do?\n",
      "    - Check if your computer is connected to the internet.\n",
      "    - If your connection is unstable you can consider working using the offline mode:\n",
      "        \u001b[96mrun = neptune.init(mode=\"offline\")\u001b[0m\n",
      "        \n",
      "You can read in detail how it works and how to upload your data on the following doc pages:\n",
      "    - https://docs.neptune.ai/advanced-user-guides/connection-modes#offline\n",
      "    - https://docs.neptune.ai/advanced-user-guides/uploading-offline-data\n",
      "    \n",
      "You may also want to check the following docs pages:\n",
      "    - https://docs.neptune.ai/advanced-user-guides/connectivity-issues\n",
      "    - https://docs.neptune.ai/advanced-user-guides/connection-modes\n",
      "    \n",
      "\u001b[92mNeed help?\u001b[0m-> https://docs.neptune.ai/getting-started/getting-help\n",
      "\n",
      "Error occurred during asynchronous operation processing: Invalid point for string series: monitoring/stderr : Text longer than 1000 characters was truncated\n",
      "Error occurred during asynchronous operation processing: Invalid point for string series: monitoring/stderr : Text longer than 1000 characters was truncated\n",
      "Exception in thread Thread-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/neptune/new/internal/backends/utils.py\", line 51, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/neptune/new/internal/backends/hosted_neptune_backend.py\", line 229, in ping_run\n",
      "    self.leaderboard_client.api.ping(experimentId=str(run_uuid)).response().result\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/bravado/http_future.py\", line 239, in response\n",
      "    six.reraise(*sys.exc_info())\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/six.py\", line 703, in reraise\n",
      "    raise value\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/bravado/http_future.py\", line 200, in response\n",
      "    swagger_result = self._get_swagger_result(incoming_response)\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/bravado/http_future.py\", line 124, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/bravado/http_future.py\", line 300, in _get_swagger_result\n",
      "    unmarshal_response(\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/bravado/http_future.py\", line 334, in unmarshal_response\n",
      "    raise_on_unexpected(incoming_response)\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/bravado/http_future.py\", line 408, in raise_on_unexpected\n",
      "    raise make_http_exception(response=http_response)\n",
      "bravado.exception.HTTPServiceUnavailable: 503 : {\"code\":503,\"errorType\":\"SERVICE_UNAVAILABLE\",\"title\":\"Service is not available. Try again later.\"}\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/neptune/new/internal/threading/daemon.py\", line 46, in run\n",
      "    self.work()\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/neptune/new/internal/utils/ping_background_job.py\", line 62, in work\n",
      "    self._run.ping()\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/neptune/new/run.py\", line 94, in ping\n",
      "    self._backend.ping_run(self._uuid)\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/neptune/new/internal/backends/utils.py\", line 92, in wrapper\n",
      "    raise NeptuneConnectionLostException() from last_exception\n",
      "neptune.new.exceptions.NeptuneConnectionLostException: \n",
      "\u001b[95m\n",
      "----NeptuneConnectionLostException---------------------------------------------------------\n",
      "\u001b[0m\n",
      "A connection to the Neptune server was lost.\n",
      "If you are using asynchronous (default) connection mode Neptune automatically switched to an offline mode and your data is being stored safely on the disk.\n",
      "You can upload it later using Neptune Command Line Interface:\n",
      "    \u001b[95mneptune sync -p workspace_name/project_name\u001b[0m\n",
      "\n",
      "What should I do?\n",
      "    - Check if your computer is connected to the internet.\n",
      "    - If your connection is unstable you can consider working using the offline mode:\n",
      "        \u001b[96mrun = neptune.init(mode=\"offline\")\u001b[0m\n",
      "        \n",
      "You can read in detail how it works and how to upload your data on the following doc pages:\n",
      "    - https://docs.neptune.ai/advanced-user-guides/connection-modes#offline\n",
      "    - https://docs.neptune.ai/advanced-user-guides/uploading-offline-data\n",
      "    \n",
      "You may also want to check the following docs pages:\n",
      "    - https://docs.neptune.ai/advanced-user-guides/connectivity-issues\n",
      "    - https://docs.neptune.ai/advanced-user-guides/connection-modes\n",
      "    \n",
      "\u001b[92mNeed help?\u001b[0m-> https://docs.neptune.ai/getting-started/getting-help\n",
      "\n",
      "Error occurred during asynchronous operation processing: Invalid point for string series: monitoring/stderr : Text longer than 1000 characters was truncated\n",
      "Error occurred during asynchronous operation processing: Invalid point for string series: monitoring/stderr : Text longer than 1000 characters was truncated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error occurred during asynchronous operation processing: Invalid point for string series: monitoring/stderr : Text longer than 1000 characters was truncated\n",
      "Error occurred during asynchronous operation processing: Invalid point for string series: monitoring/stderr : Text longer than 1000 characters was truncated\n",
      "Error occurred during asynchronous operation processing: Invalid point for string series: monitoring/stderr : Text longer than 1000 characters was truncated\n",
      "Error occurred during asynchronous operation processing: Invalid point for string series: monitoring/stderr : Text longer than 1000 characters was truncated\n",
      "Unexpected error occurred. Killing Neptune asynchronous thread. All data is safe on disk.\n",
      "Exception in thread Thread-13:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/urllib3/response.py\", line 697, in _update_chunk_length\n",
      "    self.chunk_left = int(line, 16)\n",
      "ValueError: invalid literal for int() with base 16: b''\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/urllib3/response.py\", line 438, in _error_catcher\n",
      "    yield\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/urllib3/response.py\", line 764, in read_chunked\n",
      "    self._update_chunk_length()\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/urllib3/response.py\", line 701, in _update_chunk_length\n",
      "    raise InvalidChunkLength(self, line)\n",
      "urllib3.exceptions.InvalidChunkLength: InvalidChunkLength(got length b'', 0 bytes read)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/requests/models.py\", line 753, in generate\n",
      "    for chunk in self.raw.stream(chunk_size, decode_content=True):\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/urllib3/response.py\", line 572, in stream\n",
      "    for line in self.read_chunked(amt, decode_content=decode_content):\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/urllib3/response.py\", line 793, in read_chunked\n",
      "    self._original_response.close()\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/contextlib.py\", line 131, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/urllib3/response.py\", line 455, in _error_catcher\n",
      "    raise ProtocolError(\"Connection broken: %r\" % e, e)\n",
      "urllib3.exceptions.ProtocolError: (\"Connection broken: InvalidChunkLength(got length b'', 0 bytes read)\", InvalidChunkLength(got length b'', 0 bytes read))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/neptune/new/internal/threading/daemon.py\", line 46, in run\n",
      "    self.work()\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/neptune/new/internal/operation_processors/async_operation_processor.py\", line 115, in work\n",
      "    self.process_batch(batch, version)\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/neptune/new/internal/operation_processors/async_operation_processor.py\", line 121, in process_batch\n",
      "    result = self._processor._backend.execute_operations(self._processor._run_uuid, batch)\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/neptune/new/internal/backends/hosted_neptune_backend.py\", line 252, in execute_operations\n",
      "    errors.extend(self._execute_operations(run_uuid, other_operations))\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/neptune/new/internal/backends/utils.py\", line 51, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/neptune/new/internal/backends/hosted_neptune_backend.py\", line 307, in _execute_operations\n",
      "    result = self.leaderboard_client.api.executeOperations(**kwargs).response().result\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/bravado/http_future.py\", line 197, in response\n",
      "    incoming_response = self._get_incoming_response(timeout)\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/bravado/http_future.py\", line 124, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/bravado/http_future.py\", line 291, in _get_incoming_response\n",
      "    inner_response = self.future.result(timeout=timeout)\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/bravado/requests_client.py\", line 266, in result\n",
      "    response = self.session.send(\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/requests/sessions.py\", line 697, in send\n",
      "    r.content\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/requests/models.py\", line 831, in content\n",
      "    self._content = b''.join(self.iter_content(CONTENT_CHUNK_SIZE)) or b''\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/requests/models.py\", line 756, in generate\n",
      "    raise ChunkedEncodingError(e)\n",
      "requests.exceptions.ChunkedEncodingError: (\"Connection broken: InvalidChunkLength(got length b'', 0 bytes read)\", InvalidChunkLength(got length b'', 0 bytes read))\n",
      "Exception in thread Thread-12:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/urllib3/response.py\", line 697, in _update_chunk_length\n",
      "    self.chunk_left = int(line, 16)\n",
      "ValueError: invalid literal for int() with base 16: b''\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/urllib3/response.py\", line 438, in _error_catcher\n",
      "    yield\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/urllib3/response.py\", line 764, in read_chunked\n",
      "    self._update_chunk_length()\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/urllib3/response.py\", line 701, in _update_chunk_length\n",
      "    raise InvalidChunkLength(self, line)\n",
      "urllib3.exceptions.InvalidChunkLength: InvalidChunkLength(got length b'', 0 bytes read)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/requests/models.py\", line 753, in generate\n",
      "    for chunk in self.raw.stream(chunk_size, decode_content=True):\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/urllib3/response.py\", line 572, in stream\n",
      "    for line in self.read_chunked(amt, decode_content=decode_content):\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/urllib3/response.py\", line 793, in read_chunked\n",
      "    self._original_response.close()\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/contextlib.py\", line 131, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/urllib3/response.py\", line 455, in _error_catcher\n",
      "    raise ProtocolError(\"Connection broken: %r\" % e, e)\n",
      "urllib3.exceptions.ProtocolError: (\"Connection broken: InvalidChunkLength(got length b'', 0 bytes read)\", InvalidChunkLength(got length b'', 0 bytes read))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/neptune/new/internal/threading/daemon.py\", line 46, in run\n",
      "    self.work()\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/neptune/new/internal/utils/ping_background_job.py\", line 62, in work\n",
      "    self._run.ping()\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/neptune/new/run.py\", line 94, in ping\n",
      "    self._backend.ping_run(self._uuid)\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/neptune/new/internal/backends/utils.py\", line 51, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/neptune/new/internal/backends/hosted_neptune_backend.py\", line 229, in ping_run\n",
      "    self.leaderboard_client.api.ping(experimentId=str(run_uuid)).response().result\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/bravado/http_future.py\", line 197, in response\n",
      "    incoming_response = self._get_incoming_response(timeout)\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/bravado/http_future.py\", line 124, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/bravado/http_future.py\", line 291, in _get_incoming_response\n",
      "    inner_response = self.future.result(timeout=timeout)\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/bravado/requests_client.py\", line 266, in result\n",
      "    response = self.session.send(\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/requests/sessions.py\", line 697, in send\n",
      "    r.content\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/requests/models.py\", line 831, in content\n",
      "    self._content = b''.join(self.iter_content(CONTENT_CHUNK_SIZE)) or b''\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/requests/models.py\", line 756, in generate\n",
      "    raise ChunkedEncodingError(e)\n",
      "requests.exceptions.ChunkedEncodingError: (\"Connection broken: InvalidChunkLength(got length b'', 0 bytes read)\", InvalidChunkLength(got length b'', 0 bytes read))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/urllib3/response.py\", line 697, in _update_chunk_length\n",
      "    self.chunk_left = int(line, 16)\n",
      "ValueError: invalid literal for int() with base 16: b''\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/urllib3/response.py\", line 438, in _error_catcher\n",
      "    yield\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/urllib3/response.py\", line 764, in read_chunked\n",
      "    self._update_chunk_length()\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/urllib3/response.py\", line 701, in _update_chunk_length\n",
      "    raise InvalidChunkLength(self, line)\n",
      "urllib3.exceptions.InvalidChunkLength: InvalidChunkLength(got length b'', 0 bytes read)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/requests/models.py\", line 753, in generate\n",
      "    for chunk in self.raw.stream(chunk_size, decode_content=True):\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/urllib3/response.py\", line 572, in stream\n",
      "    for line in self.read_chunked(amt, decode_content=decode_content):\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/urllib3/response.py\", line 793, in read_chunked\n",
      "    self._original_response.close()\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/contextlib.py\", line 131, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/urllib3/response.py\", line 455, in _error_catcher\n",
      "    raise ProtocolError(\"Connection broken: %r\" % e, e)\n",
      "urllib3.exceptions.ProtocolError: (\"Connection broken: InvalidChunkLength(got length b'', 0 bytes read)\", InvalidChunkLength(got length b'', 0 bytes read))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/neptune/new/internal/threading/daemon.py\", line 46, in run\n",
      "    self.work()\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/neptune/new/internal/utils/ping_background_job.py\", line 62, in work\n",
      "    self._run.ping()\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/neptune/new/run.py\", line 94, in ping\n",
      "    self._backend.ping_run(self._uuid)\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/neptune/new/internal/backends/utils.py\", line 51, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/neptune/new/internal/backends/hosted_neptune_backend.py\", line 229, in ping_run\n",
      "    self.leaderboard_client.api.ping(experimentId=str(run_uuid)).response().result\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/bravado/http_future.py\", line 197, in response\n",
      "    incoming_response = self._get_incoming_response(timeout)\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/bravado/http_future.py\", line 124, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/bravado/http_future.py\", line 291, in _get_incoming_response\n",
      "    inner_response = self.future.result(timeout=timeout)\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/bravado/requests_client.py\", line 266, in result\n",
      "    response = self.session.send(\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/requests/sessions.py\", line 697, in send\n",
      "    r.content\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/requests/models.py\", line 831, in content\n",
      "    self._content = b''.join(self.iter_content(CONTENT_CHUNK_SIZE)) or b''\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/requests/models.py\", line 756, in generate\n",
      "    raise ChunkedEncodingError(e)\n",
      "requests.exceptions.ChunkedEncodingError: (\"Connection broken: InvalidChunkLength(got length b'', 0 bytes read)\", InvalidChunkLength(got length b'', 0 bytes read))\n",
      "Experiencing connection interruptions. Reestablishing communication with Neptune.\n",
      "Experiencing connection interruptions. Reestablishing communication with Neptune.\n",
      "Experiencing connection interruptions. Reestablishing communication with Neptune.\n",
      "Experiencing connection interruptions. Reestablishing communication with Neptune.\n",
      "Experiencing connection interruptions. Reestablishing communication with Neptune.\n",
      "Experiencing connection interruptions. Reestablishing communication with Neptune.\n",
      "Experiencing connection interruptions. Reestablishing communication with Neptune.\n",
      "Experiencing connection interruptions. Reestablishing communication with Neptune.\n",
      "Experiencing connection interruptions. Reestablishing communication with Neptune.\n",
      "Experiencing connection interruptions. Reestablishing communication with Neptune.\n",
      "Experiencing connection interruptions. Reestablishing communication with Neptune.\n",
      "Unexpected error occurred. Killing Neptune asynchronous thread. All data is safe on disk.\n",
      "Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/neptune/new/internal/threading/daemon.py\", line 46, in run\n",
      "    self.work()\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/neptune/new/internal/operation_processors/async_operation_processor.py\", line 115, in work\n",
      "    self.process_batch(batch, version)\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/neptune/new/internal/operation_processors/async_operation_processor.py\", line 121, in process_batch\n",
      "    result = self._processor._backend.execute_operations(self._processor._run_uuid, batch)\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/neptune/new/internal/backends/hosted_neptune_backend.py\", line 252, in execute_operations\n",
      "    errors.extend(self._execute_operations(run_uuid, other_operations))\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/neptune/new/internal/backends/utils.py\", line 51, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/neptune/new/internal/backends/hosted_neptune_backend.py\", line 307, in _execute_operations\n",
      "    result = self.leaderboard_client.api.executeOperations(**kwargs).response().result\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/bravado/client.py\", line 276, in __call__\n",
      "    return http_client.request(\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/bravado/requests_client.py\", line 399, in request\n",
      "    self.authenticated_request(sanitized_params),\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/bravado/requests_client.py\", line 440, in authenticated_request\n",
      "    return self.apply_authentication(requests.Request(**request_params))\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/bravado/requests_client.py\", line 445, in apply_authentication\n",
      "    return self.authenticator.apply(request)\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/neptune/oauth.py\", line 118, in apply\n",
      "    self.auth.refresh_token_if_needed()\n",
      "  File \"/Users/ldalessi/anaconda3/envs/pyro/lib/python3.8/site-packages/neptune/utils.py\", line 330, in wrapper\n",
      "    raise ConnectionLost()\n",
      "neptune.api_exceptions.ConnectionLost: Connection lost. Please try again.\n"
     ]
    }
   ],
   "source": [
    "print(expected_final_logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "neptune": {
   "notebookId": "ea58cca6-5f9c-4037-8151-a3c4003e7b30"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
